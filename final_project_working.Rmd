---
title: "Your Experience or looks – What matters most in LinkedIn?"
author: "Bryan Morgan, Alan Tan and Debasish Mukhopadhyay"
date: "4/3/2019"
output:
  pdf_document: default
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
**************
* Suggested Format: 
- Statement of the research question; why this is interesting to
  someone who is predisposed to be interested; what other people know
  about this; what (if any) obsrevational work has to say about this question
- Clear statement of the experiment
  - Research Design (using =ROXO= grammar)
  - Randomization engineering 
  - Experimental materials (e.g. treatment materials)
  - Measurement of variables
  - Modeling choices
- Clear statement of results
  - In text description of your results
  - Figures and tables that support your in text description
  - Clean, clear, well articulated relationships between your theory,
    your hypotheses, the numbers that your models produce, and the
    figures you present. 
*********************
## Background
LinkedIn has been the ubiquitous platform for the professional networking and job search. With 610 million users, 146 million in USA alone, makes it single most important profesional networking place in the world. LinkedIn is extremely popular globally. It is present in more than 200 countries. 40% of it's users visit LinkedIn site daily. One may ask, why do these people come to LinkedIn. It is now a survival tool for professionals in a highly connected World. Here different professionals learn about what is happenning in their industry and profession and learn from others. 80% of LinkedIn members consider professional networking is key to their career success. https://blog.linkedin.com/2017/april/13/introducing-a-smarter-way-to-message-and-build-meaningful-relationships-on-linkedin It is 

A typical user of LinkedIn creates her digital resume, called "profile" and create a graph of connections with friends, coleagues and acquantances. An user with 1000 connections will potentially have more than 11 million connection in a job market. WhileUsers have an option to include a photograph in their profile. LinkedIn research shows that profiles with a headshot gets 21 times more profile views compared to profiles without pictures https://blog.linkedin.com/2016/08/03/5-steps-to-improve-your-linkedin-profile-in-minutes-. Same research also suggests that having a profile picture increases your chance to receive a message by 36 times. 

An accepted rule in LinkedIn profile is to have a very professional picture. A simple web search shows more than 120 websites suggests various rules for LinkedIn profile images. They all claim – Image quality matters. However no scientific research has been done so far to prove that to be the case. As a part of the W241 course, we wanted to run a scientific experiment to prove if it really matters to have professional pictures in your profile.

## Research Question
In a connected world of data and algorithms, does your look matter in your professional social network and employment?

We are setting up an experiment where want to test this. In our experiment, the null hypothesis is that the quality of profile picture does not matter. 

## Why is this research is important
We know that a picture worth a thousands words. In your picture, you show Your physical, psychological, economical, environmental conditions through your presentation, gesture, dress, background,expressions etc. A person, or ML algorithms can guess your race, religion, location [GPS coordinates in metadata], age, sexual orientation etc. 

## Research Design

For our research design, we ended up with two very different experiments. One of which ran completely, and another which we initially started with

### Null Hypothesis

### Treatment and control

### Subjects 

### Randamization Strategy
3 members of the team each reached out to 10-12 friends and recruited them as our volunteers (32 in total).  We each randomly selected half of our volunteers as control using random functions in Excel and R, the other half are assigned as treatment. 

We asked our volunteers each monitor and report their profile views to us.  This is to avoid unintended affect to number of views (our view of their profile would have artificially increased their profile views.)

The half of volunteers assigned to treatment are asked to change their profile picture to a worse version according to previous description on the 14th day of our experiment. (16 people are in treatment and all changed their profile pictures on that day.) 

Blocking:
The reason each one of us randomly selected half of our volunteers as treatment is we believe blocking is needed.  The 3 team members are from three different geographical localtions (the Bay area, Seattle, and Toronto), and we have slightly different connections in LinkedIn.  We anticipate the effect of profile pictures could be different based on region, or job categories, or LinkedIn active level.  By blocking by "volunteer contact", we hope to limit the impact of differences between groups of people (for example some team member's contacts have in average much higher profile views than others)

### Measurement Process
We measuree "number of profile views" as our outcome. 

A view is when a LinkedIn user clicks on a the profile brief to open the profile details. 
LinkedIn people search result shows this profile briefs, with profile picture in a list format.  A user will click on this profile brief is they are interested in a particular profile, and view more detailed information for the person.  Because the profile picture is shown with name, headline, and title shown, so we anticipate profile picture is an important part of the information affecting a viewer's decision to click on the brief or not. 

Sources of profile views might include, but not limited to: Recruiters looking for candidate, or industry insider looking for contact, employer, or mentor, it could be just a google user searching for someone with similar names or background.

A complication is search engines might serve direct link to a profile detail page by passing the profile brief list, where profile picture is part of the decision to view detail or not, which can be a concern for real user profiles, especially those influencers on LinkedIn. 

To address this concern, we take 2 weeks of "before" and 2 weeks "after" profile pic changes, then compare the differences of before and after.  Because the search engie originated views are not affected by the profile pictures (search engines do not show profile pictures), so the differences between the two periods would not affected by such views.  
So, we took a Difference in Different approach, to remove the influence of the search engine originated views. 

DnD = Before_views – After_views



### ROXO 
Considering the possibilities of substantially different baselines we took a difference in difference approach
We felt this would help mitigate potential noise and help us isolate any potential treatment effect
We took 10+ individuals each, and randomly assigned them to control or treatment
From there we observed 2 weeks of their LinkedIn data, prior to the experiment
We then administered treatment, confirmed compliance and then observed again after 2 weeks

### Pilot and learning

## Analysis 
```{r echo=FALSE}
library(stargazer)
library(multiwayvcov)  
library(sandwich)
library(ri)
library(dplyr)
library(pwr)
library(psych)
library(lmtest)

main_dataFrame <- read.csv("/Users/bmorgan/Downloads/Results\ For\ Final\ Project\ -\ Sheet1.csv")
main_dataFrame$DnD <- (main_dataFrame$week3+main_dataFrame$week4) - (main_dataFrame$week1+main_dataFrame$week2)
```
#### Randomized Inference Results
```{r echo=FALSE}
ri_y <- main_dataFrame$DnD
ri_Z <- main_dataFrame$Treatment
ri_block <- main_dataFrame$Source

perms <- genperms(ri_Z, blockvar = ri_block)
probs <- genprobexact(ri_Z, blockvar = ri_block)
ate <- estate(ri_y, ri_Z, prob = probs)

Ys <- genouts(ri_y, ri_Z, ate=0)
distout <- gendist(Ys, perms, prob = probs)
dispdist(distout, ate)
```

```{r echo=FALSE}
main_lm_nieve <- lm(DnD ~ Treatment + Source, data=main_dataFrame)
main_lm_nieve_summary <- summary(main_lm_nieve)
main_lm_nieve_summary
```

```{r echo=FALSE}
stargazer(coeftest(main_lm_nieve,vcov.=vcovHC(main_lm_nieve,type="HC1")), type = "text")
```

```{r echo=FALSE}
main_lm_covariate <- lm(DnD ~ Treatment + Female + Highest.Education + Age + Source, data=main_dataFrame)
main_lm_covariate_summary <- summary(main_lm_covariate)
main_lm_covariate_summary
```

```{r}
stargazer(coeftest(main_lm_covariate,vcov.=vcovHC(main_lm_covariate,type="HC1")), type = "text")
```

```{r}
control_mean <- mean(main_dataFrame$DnD[main_dataFrame$Treatment == 0])
treatment_mean <- mean(main_dataFrame$DnD[main_dataFrame$Treatment == 1])
sample_std <- sd(main_dataFrame$DnD)
max_dnd <- max(main_dataFrame$DnD)
min_dnd <- min(main_dataFrame$DnD)
describe(main_dataFrame$DnD)

#Basic info calculations
sample_std
control_mean
treatment_mean
max_dnd
min_dnd
anova(main_lm_covariate)
```

```{r}
#Calculate Power
pwr.f2.test(u =8, v =23, f2 =(0.2161/(1-0.2161)) , sig.level = 0.05)
```
## Results
We intentionally designed the experiment to try and compensate for the fact that individuals may have significantly different baselines with regards to profiles views. We subsequently used the difference-in-difference methodology to ensure we were looking at the changes in profile views for individual users, somewhat normalizing the treatment effect. We expected to mitigate wild shifts in profiles views with this methodology, and expected that we would be able to sufficiently satisfy the assumption that a change in the control group would have been equally likely to impact the treatment group. Unfortunately though, the difference-in-difference method was not sufficient to compensate for the wide changes in profile views an individual will see from week to week. This was born out in our data, resulting in a wide range, with a minimum of -53 profile views (meaning the two weeks prior to the experiment, this individual had 53 more profile views than during the two weeks of the experiment) and a maximum of +23 profile views. The results of our experiment, were therefore inconclusive. Due to the wide range of results, even using difference in difference, we were not able to detect a treatment effect. Ultimately, we would have needed a much larger sample size to have made any significant conclusions. Nonetheless, we were diligent in our analysis, and examined the data using both randomized inference to initially evaluate whether or not there was any treatment effect, and linear regression with covariates to try and isolate more of the potential treatment effect.
When conducting the randomized inference analysis, we wanted to ensure we were taking into consideration some potential bias or unobserved heterogeneous bias that may have developed due to difference in the individuals approached. The three co-authors come from very different backgrounds, with different regions, levels of seniority in their organizations and social backgrounds. This meant, that we should expect there to be a non-trivial amount of unobserved heterogeneity amongst the groups, so we we decided to block the subjects based on the source. For privacy reasons, we converted the original names of both the sources (with sources becoming “Source 1”,  “Source 2”,  “Source 3”) as well as the subjects (using SHA1, via the hashlib library in python, to convert their initials). From there, we were able to start running the actual randomized inference. We wanted to ensure that our experimental results were likely to generalize well and to follow common conventions, so predetermined that we would use a p-value of 0.05 as our requirement for statistically significant values. Unfortunately our results from the randomized inference, using blocking, did not meet this criteria. We used a two tailed method because, while we expected the unprofessional photos to have a negative impact on profile views, we were not certain of the impact and wanted to evaluate at a larger level whether or not there was any effect at all. Our two tailed test had a  2.5% quantile equal to -8.8125 and a 97.5% quantile equal to 8.6875 while our average treatment effect was 7.0625. This resulted in a p-value of 0.135 for our two-tailed test. As identified earlier, this did not give us a statistically significant test, but it does provide some interesting material. Subsequently we decided to dig a little deeper and evaluate the results using linear regression as well.
Using linear regression, we wanted to take a two step approach. First we wanted to do something similar to what we had done in the randomized inference with regards to blocking. Secondly we wanted to add additional covariates which we thought may be interesting or contribute to the wide variety of difference-in-difference results we were seeing. Starting from the initial analysis, using the source as a covariate, we fitted a linear model to our data. The resulting model did identify with a p-value less than 0.05 that one of the sources correlated with a lower difference-in-difference. Yet it is impossible to evaluate which of the many possible unobserved sources of heterogeneity caused the difference as there are far many more factors that could be different about the sources than what we were able to capture. Nonetheless, after seeing these results we decided to evaluate the experiment with greater detail, with the covariates we were able to collect from our subjects.
The second phase of our analysis using linear regression took into consideration three additional covariates which we expected to play a potentially significant role in profile views: sex, education and age. For sex, we encoded the data with a dummy variable called Female with 1 being true and 0 being false. For education, we used categorical variables ranging from HIgh School, to PhD (with Bachelor and Masters in between). Finally for Age we used decades, as we did not want to violate the privacy of our participants and did not expect the difference between individual years to be significant. With this coding in place, we fitted a new model to the difference-in-difference data and evaluated the impact of treatment, while trying to control for these other covariates. One of the first effects of including the other covariates was that it significantly altered the impact of the sources, with Source 3 now having a lesser negative impact with a magnitude of -4.524 and SE of 6.529. Additionally, this resulted in an even weaker average treatment effect, which had a magnitude of 3.572 and robust standard errors of 4.244. This effectively meant that we could not identify any meaningful impact of the treatment on the number of profile views an individual was likely to receive during the two week experiment period. 
Interestingly though, we did find some correlation between being female and only having a high-school education. Being Female, there seemed to be an increased number of views generally, with a effect of 10.225 and robust SE of 4.378. Based on the data though, it is unclear why women would be receiving more views on LinkedIn, and any conclusions would need to be backed up by an independent experiment. This is because there was a relatively small number of women in the experiment, so any conclusions could be simply due to chance. Similarly, it seems that having only a high-school education has a negative impact on profile views, but considering there was only one individual with only a high-school education, it is impossible to draw any conclusions. Ultimately, we would need a much larger and more diverse sample in order to support either of these findings with any confidence, though they do suggest some potential areas for additional consideration in the future. 

## Generalizability
Considering the possibilities of substantially different baselines we took a difference in difference approach
We felt this would help mitigate potential noise and help us isolate any potential treatment effect
We took 10+ individuals each, and randomly assigned them to control or treatment
From there we observed 2 weeks of their LinkedIn data, prior to the experiment
We then administered treatment, confirmed compliance and then observed again after 2 weeks

However, our sample size is very small compare to how many co-variates LinkedIn collect people's information, and ways to categorize its users. 
We need much more volunteers to control for factors like 
-level of activeness on LinkedIn introduces lots of variance
-gendere/age
-career status/seniority
-nature of work
-industry

We also need a standarization approach to "make a picture bad".  While there are many qualitative discussions of what is a good .vs. bad profile picture, an objective standard is needed to be able to generalize our findings.  



## Conclusion
Use a paid campaign in LinkedIn to recruite random volunteers with random backgrounds to avoid selection bias

Run the experiment for multiple weeks so that each volunteer can have multiple treatments 

Use automation to create treatment "dosage"

Collect more covariate  information to better understand variability around professionals such as bloggers, writers who likely to have high variability in profile views



